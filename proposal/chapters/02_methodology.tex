\chapter*{Methodology}\label{chapter:methodology}

ChristBERT will be built upon the foundation provided by the State-of-the-Art
German language model GeistBERT. GeistBERT is an improved version of GottBERT
leveraging a more diverse and larger training data as well as recent
developments in pre-training methodologies such as Whole Word Masking (WWM). We
will utilize the Byte-Pair Encoding (BPE) vocabulary introduced in
GottBERT as a starting point, a tokenizer relying on subword units which are
extracted by performing statistical analysis of the training corpus.

Constructing ChristBERT will involve pre-training models from scratch
initialized with GeistBERT's checkpoints, ensuring that it is tailored to the
specific needs of the medical domain.

A critical aspect of this development will be the creation of a domain-suitable
vocabulary. The vocabulary size is a hyperparameter and  will be carefully
determined, with reference to the vocabulary sizes of 44k, 52k (used by
GottBERT) and 60k. This will ensure that the model is both comprehensive in its
understanding of medical language and efficient in processing it.

Consequently, we will be pre-training and comparing 4 different models in total
with the following configurations:

\begin{itemize}
    \item Based on GeistBERT using GottBERT vocabulary as a baseline
    \item From scratch using vocabulary sizes: 44k, 52k and 60k
\end{itemize}

Pre-Training objective on all configurations will be masked language modelling
(MLM) with dynamically and whole word masked (WWM) inputs. Training parameters
will be also adapted from GeistBERT's configuration.

\subsection*{Training Data}

Corpus creation will play a central role in the development of ChristBERT. The
corpus will be composed of a diverse array of sources, including articles from
information service provider for healthcare professionals Hpsmedia, German
medical articles from Wikipedia, PubMed abstracts and Springer Nature articles.
These sources will provide a broad spectrum of medical language data, ensuring
the model's robustness and applicability. To enhance the diversity and coverage
of the corpus, translation augmentation will be employed, allowing for the
inclusion of texts that may not originally be in German. The crucial and
challenging part of this thesis will be acquiring Springer Nature data with
their API. Furthermore, if size is large enough, the corpus will integrate
terminologies from SNOMED and scientific abstracts with biological and medical
relevance from the LIVIVO seach engine.

In summary, the following sources will be used for corpus creation:

\begin{itemize}
    \item Medical Wikipedia
    \item Hpsmedia Publishing Group
    \item PubMed abstracts
    \item Springer Nature
    \item optionally: SNOMED medical ontology
    \item optionally: Abstracts from LIVIVO search engine
\end{itemize}

\subsection*{Evaluation}

ChristBERT will be evaluated in a rigorous process, leveraging existing
codebases and evaluation pipelines where applicable. The model's performance
will be benchmarked on a variety of downstream tasks after being fine-tuned on
them respectively and compared with other state-of-the-art BERT models in the
German medical domain. We will be using experiments analogous to those conducted
for medBERT.de~\cite{bressem2024medbert}, as detailed on their Hugging Face
page. A grid search similar to the one from GottBERT will be conducted to
optimize the model's parameters focusing on batch size and learning rate. 

The evaluation will include assessments on datasets such as
BRONCO150~\cite{kittner2021bronco150}, GGPONC 2.0~\cite{borchert2022ggponc},
MANTRA GSC~\cite{kors2015mantragsc}, GERNERMED++~\cite{frei2022gernermed}, and
GPTNERMED~\cite{frei2023gptnermed}, ensuring that ChristBERT's performance is
thoroughly vetted across a range of relevant medical tasks. Additionally, the
i2c2 benchmark for smoker status classification, will be adapted into German for
testing ChristBERT.
