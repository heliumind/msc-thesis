@article{scheible2020gottbert,
  title={Gottbert: a pure german language model},
  author={Scheible, Raphael and Thomczyk, Fabian and Tippmann, Patric and Jaravine, Victor and Boeker, Martin},
  journal={arXiv preprint arXiv:2012.02110},
  year={2020}
}

@article{bressem2024medbert,
  title={Medbert. de: A comprehensive german bert model for the medical domain},
  author={Bressem, Keno K and Papaioannou, Jens-Michalis and Grundmann, Paul and Borchert, Florian and Adams, Lisa C and Liu, Leonhard and Busch, Felix and Xu, Lina and Loyen, Jan P and Niehues, Stefan M and others},
  journal={Expert Systems with Applications},
  volume={237},
  pages={121598},
  year={2024},
  publisher={Elsevier}
}

@article{muller2017livivo,
  title={Livivo--the vertical search engine for life sciences},
  author={M{\"u}ller, Bernd and Poley, Christoph and P{\"o}ssel, Jana and Hagelstein, Alexandra and G{\"u}bitz, Thomas},
  journal={Datenbank-Spektrum},
  volume={17},
  pages={29--34},
  year={2017},
  publisher={Springer}
}

@article{kittner2021bronco150,
  title={Annotation and initial evaluation of a large annotated German oncological corpus},
  author={Kittner, Madeleine and Lamping, Mario and Rieke, Damian T and G{\"o}tze, Julian and Bajwa, Bariya and Jelas, Ivan and R{\"u}ter, Gina and Hautow, Hanjo and S{\"a}nger, Mario and Habibi, Maryam and others},
  journal={JAMIA open},
  volume={4},
  number={2},
  pages={ooab025},
  year={2021},
  publisher={Oxford University Press}
}

@inproceedings{borchert2022ggponc,
  title={GGPONC 2.0-the German clinical guideline corpus for oncology: Curation workflow, annotation policy, baseline NER taggers},
  author={Borchert, Florian and Lohr, Christina and Modersohn, Luise and Witt, Jonas and Langer, Thomas and Follmann, Markus and Gietzelt, Matthias and Arnrich, Bert and Hahn, Udo and Schapranow, Matthieu-P},
  booktitle={Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  pages={3650--3660},
  year={2022}
}

@article{kors2015mantragsc,
  title={A multilingual gold-standard corpus for biomedical concept recognition: the Mantra GSC},
  author={Kors, Jan A and Clematide, Simon and Akhondi, Saber A and Van Mulligen, Erik M and Rebholz-Schuhmann, Dietrich},
  journal={Journal of the American Medical Informatics Association},
  volume={22},
  number={5},
  pages={948--956},
  year={2015},
  publisher={Oxford University Press}
}

@article{frei2022gernermed,
  title={GERNERMED: An open German medical NER model},
  author={Frei, Johann and Kramer, Frank},
  journal={Software Impacts},
  volume={11},
  pages={100212},
  year={2022},
  publisher={Elsevier}
}

@article{frei2023gptnermed,
  title={Annotated dataset creation through large language models for non-english medical NLP},
  author={Frei, Johann and Kramer, Frank},
  journal={Journal of Biomedical Informatics},
  volume={145},
  pages={104478},
  year={2023},
  publisher={Elsevier}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@inproceedings{ott2019fairseq,
  title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages={48--53},
  year={2019}
}

@article{touvron2023llama1,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zowalla2020crawling,
  title={Crawling the german health web: Exploratory study and graph analysis},
  author={Zowalla, Richard and Wetter, Thomas and Pfeifer, Daniel},
  journal={Journal of medical Internet research},
  volume={22},
  number={7},
  pages={e17853},
  year={2020},
  publisher={JMIR Publications Toronto, Canada}
}

@inproceedings{peng2019transfer,
  title={Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets},
  author={Peng, Yifan and Yan, Shankai and Lu, Zhiyong},
  booktitle={Proceedings of the 18th BioNLP Workshop and Shared Task},
  pages={58--65},
  year={2019}
}

@inproceedings{beltagy2019scibert,
  title={SciBERT: A Pretrained Language Model for Scientific Text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3615--3620},
  year={2019}
}

@article{huang2019clinicalbert,
  title={Clinicalbert: Modeling clinical notes and predicting hospital readmission},
  author={Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
  journal={arXiv preprint arXiv:1904.05342},
  year={2019}
}

@inproceedings{martin2020camembert,
	address = {Online},
	title = {{CamemBERT}: a {Tasty} {French} {Language} {Model}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.645},
	abstract = {Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models –in all languages except English– very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Martin, Louis and Muller, Benjamin and Ortiz Suárez, Pedro Javier and Dupont, Yoann and Romary, Laurent and de la Clergerie, Éric and Seddah, Djamé and Sagot, Benoît},
	month = jul,
	year = {2020},
	pages = {7203--7219}
}

@inproceedings{chan2020german,
  title={German's Next Language Model},
  author={Chan, Branden and Schweter, Stefan and M{\"o}ller, Timo},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={6788--6796},
  year={2020}
}

@article{lentzen2022critical,
  title={Critical assessment of transformer-based AI models for German clinical notes},
  author={Lentzen, Manuel and Madan, Sumit and Lage-Rupprecht, Vanessa and K{\"u}hnel, Lisa and Fluck, Juliane and Jacobs, Marc and Mittermaier, Mirja and Witzenrath, Martin and Brunecker, Peter and Hofmann-Apitius, Martin and others},
  journal={JAMIA open},
  volume={5},
  number={4},
  pages={ooac087},
  year={2022},
  publisher={Oxford University Press}
}

@article{arefeva2022tourbert,
  title={When BERT Started Traveling: TourBERT—A Natural Language Processing Model for the Travel Industry},
  author={Arefeva, Veronika and Egger, Roman},
  journal={Digital},
  volume={2},
  number={4},
  pages={546--559},
  year={2022},
  publisher={MDPI}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@inproceedings{ng2019facebook,
  title={Facebook FAIR's WMT19 News Translation Task Submission},
  author={Ng, Nathan and Yee, Kyra and Baevski, Alexei and Ott, Myle and Auli, Michael and Edunov, Sergey},
  booktitle={Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
  year={2019},
  organization={Association for Computational Linguistics}
}

@inproceedings{edunov2018understanding,
  title={Understanding Back-Translation at Scale},
  author={Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={489--500},
  year={2018}
}

@article{johnson2023mimic,
  title={MIMIC-IV, a freely accessible electronic health record dataset},
  author={Johnson, Alistair EW and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J and Hao, Sicheng and Moody, Benjamin and Gow, Brian and others},
  journal={Scientific data},
  volume={10},
  number={1},
  pages={1},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{toraman2023impact,
  title={Impact of tokenization on language models: An analysis for turkish},
  author={Toraman, Cagri and Yilmaz, Eyup Halit and {\c{S}}ahinu{\c{c}}, Furkan and Ozcelik, Oguzhan},
  journal={ACM Transactions on Asian and Low-Resource Language Information Processing},
  volume={22},
  number={4},
  pages={1--21},
  year={2023},
  publisher={ACM New York, NY}
}
